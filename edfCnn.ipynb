{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import ntpath\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import edfreader\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyeeg\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from mne.datasets.sleep_physionet._utils import _fetch_one, _data_path, AGE_SLEEP_RECORDS\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, log_loss\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "VBS = True  # constant boolean to enable/disbale verbose\n",
    "EPOCH_SEC_SIZE = 30  # Epoch duration selection\n",
    "seed = 42  # seed value for the random seeds\n",
    "batch_size = 64\n",
    "number_of_subj = 50\n",
    "\n",
    "# values to label the stages\n",
    "UNKNOWN = -1\n",
    "W = 0\n",
    "N1 = 1\n",
    "N2 = 2\n",
    "N3 = 3\n",
    "REM = 4\n",
    "\n",
    "# making string dictionary for the label values\n",
    "label_dict = {\n",
    "    \"UNKNOWN\"  : UNKNOWN,\n",
    "    \"W\"        : W,\n",
    "    \"N1\"       : N1,\n",
    "    \"N2\"       : N2,\n",
    "    \"N3\"       : N3,\n",
    "    \"REM\"      : REM\n",
    "}\n",
    "\n",
    "# converting from label values to strings\n",
    "class_dict = {\n",
    "    -1: \"UNKNOWN\",\n",
    "    0 : \"W\",\n",
    "    1 : \"N1\",\n",
    "    2 : \"N2\",\n",
    "    3 : \"N3\",\n",
    "    4 : \"REM\"\n",
    "}\n",
    "\n",
    "# annotation dictionary to convert from string to label values\n",
    "annot2label = {\n",
    "    \"Sleep stage ?\": -1,\n",
    "    \"Movement time\": -1,\n",
    "    \"Sleep stage W\": 0,\n",
    "    \"Sleep stage 1\": 1,\n",
    "    \"Sleep stage 2\": 2,\n",
    "    \"Sleep stage 3\": 3,\n",
    "    \"Sleep stage 4\": 3,\n",
    "    \"Sleep stage R\": 4\n",
    "}\n",
    "project_path = os.path.abspath(os.getcwd())  # finding the current project path in windows"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# localized fetch_data function by using mne library\n",
    "# https://github.com/mne-tools/mne-python/blob/maint/0.20/mne/datasets/sleep_physionet/age.py#L18-L111\n",
    "data_path = _data_path\n",
    "BASE_URL = 'https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette/'\n",
    "def fetch_data(subjects, recording=[1, 2], path=None, force_update=False,\n",
    "               update_path=None, base_url=BASE_URL,\n",
    "               verbose=None):  # noqa: D301\n",
    "    records = np.loadtxt(AGE_SLEEP_RECORDS,\n",
    "                         skiprows=1,\n",
    "                         delimiter=',',\n",
    "                         usecols=(0, 1, 2, 6, 7),\n",
    "                         dtype={'names': ('subject', 'record', 'type', 'sha',\n",
    "                                          'fname'),\n",
    "                                'formats': ('<i2', 'i1', '<S9', 'S40', '<S22')}\n",
    "                         )\n",
    "    psg_records = records[np.where(records['type'] == b'PSG')]\n",
    "    hyp_records = records[np.where(records['type'] == b'Hypnogram')]\n",
    "\n",
    "    path = data_path(path=path, update_path=update_path)\n",
    "    params = [path, force_update, base_url]\n",
    "    fnames = []\n",
    "    for subject in subjects:\n",
    "        for idx in np.where(psg_records['subject'] == subject)[0]:\n",
    "            if psg_records['record'][idx] in recording:\n",
    "                psg_fname = _fetch_one(psg_records['fname'][idx].decode(),\n",
    "                                       psg_records['sha'][idx].decode(),\n",
    "                                       *params)\n",
    "                hyp_fname = _fetch_one(hyp_records['fname'][idx].decode(),\n",
    "                                       hyp_records['sha'][idx].decode(),\n",
    "                                       *params)\n",
    "                fnames.append([psg_fname, hyp_fname])\n",
    "\n",
    "    return fnames"
   ],
   "id": "d1d425ffd8a22a1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "subjects_list = []  # list to keep the address of the subject data\n",
    "except_sub = [13, 36, 52]  # omitting the subjects with incomplete data\n",
    "\n",
    "for i in range(83):\n",
    "    if i in except_sub:\n",
    "        continue\n",
    "    subjects_list.append(i)\n",
    "\n",
    "# fetching data of each subject and\n",
    "subject_files = fetch_data(subjects=subjects_list, recording=[1, 2], path= project_path)\n",
    "mapping = {'EOG horizontal': 'eog',\n",
    "           'Resp oro-nasal': 'misc',\n",
    "           'EMG submental': 'misc',\n",
    "           'Temp rectal': 'misc',\n",
    "           'Event marker': 'misc'}"
   ],
   "id": "acbea0a18f32f5cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ch_labels = 'EEG Fpz-Cz'  # channels to be selected\n",
    "data_frames = []\n",
    "if VBS:\n",
    "    print(\"Importing data into dataframes:\")\n",
    "output_path = os.path.join(project_path, \"NPZ_files\")  # path to save the npz files\n",
    "# loop to preprocess input data and save the results in npz files to be used in our models later\n",
    "for item in tqdm(subject_files):\n",
    "    filename = ntpath.basename(item[0]).replace(\"-PSG.edf\", \".npz\")  # reading the PSG files\n",
    "    if not os.path.exists(os.path.join(output_path, filename)):\n",
    "        raw_train = mne.io.read_raw_edf(item[0], verbose=VBS)\n",
    "        sampling_rate = raw_train.info['sfreq']\n",
    "        raw_ch_df = raw_train.to_data_frame(scaling_time=100.0)[ch_labels]\n",
    "        raw_ch_df = raw_ch_df.to_frame()\n",
    "        raw_ch_df.set_index(np.arange(len(raw_ch_df)))\n",
    "\n",
    "        # reading the raw headers using the EDFReader function from edfreader\n",
    "        f = open(item[0], 'r', errors='ignore', encoding='utf-8')\n",
    "        head_raw_read = edfreader.BaseEDFReader(f)\n",
    "        head_raw_read.read_header()\n",
    "        head_raw = head_raw_read.header\n",
    "        f.close()\n",
    "        raw_start_time = datetime.strptime(head_raw['date_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # read annotations from hypnogram file\n",
    "        f = open(item[1], 'r')\n",
    "        annot_raw_read = edfreader.BaseEDFReader(f)\n",
    "        annot_raw_read.read_header()\n",
    "        annot_raw = annot_raw_read.header\n",
    "        temp, temp, total_annot = zip(*annot_raw_read.records())\n",
    "        f.close()\n",
    "        annot_start_time = datetime.strptime(annot_raw['date_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        assert raw_start_time == annot_start_time  # making sure that the PSG files and hypnogram files are in sync\n",
    "        remove_idx = []    # list to keep the indicies of data that will be removed\n",
    "        labels = []        # list to keep the indicies of data that have labels\n",
    "        label_idx = []\n",
    "\n",
    "        # selecting the indicies of known labels and adding the rest to remove_idx list\n",
    "        for annot in total_annot[0]:\n",
    "            onset_sec, duration_sec, annot_char = annot\n",
    "            annot_str = \"\".join(annot_char)\n",
    "            label = annot2label[annot_str]\n",
    "            if label != UNKNOWN:\n",
    "                if duration_sec % EPOCH_SEC_SIZE != 0:\n",
    "                    raise Exception(\"Please choose anothe epoch duration!\")\n",
    "                duration_epoch = int(duration_sec / EPOCH_SEC_SIZE)\n",
    "                label_epoch = np.ones(duration_epoch, dtype=np.int) * label\n",
    "                labels.append(label_epoch)\n",
    "                idx = int(onset_sec * sampling_rate) + np.arange(duration_sec * sampling_rate, dtype=np.int)\n",
    "                label_idx.append(idx)\n",
    "            else:\n",
    "                idx = int(onset_sec * sampling_rate) + np.arange(duration_sec * sampling_rate, dtype=np.int)\n",
    "                remove_idx.append(idx)\n",
    "        labels = np.hstack(labels)\n",
    "        if len(remove_idx) > 0:\n",
    "            remove_idx = np.hstack(remove_idx)\n",
    "            select_idx = np.setdiff1d(np.arange(len(raw_ch_df)), remove_idx)\n",
    "        else:\n",
    "            select_idx = np.arange(len(raw_ch_df))\n",
    "\n",
    "        # filtering data with labels only\n",
    "        label_idx = np.hstack(label_idx)\n",
    "        select_idx = np.intersect1d(select_idx, label_idx)\n",
    "\n",
    "        # removing extra indicies\n",
    "        if len(label_idx) > len(select_idx):\n",
    "            extra_idx = np.setdiff1d(label_idx, select_idx)\n",
    "            # trimming the tail\n",
    "            if np.all(extra_idx > select_idx[-1]):\n",
    "                n_trims = len(select_idx) % int(EPOCH_SEC_SIZE * sampling_rate)\n",
    "                n_label_trims = int(math.ceil(n_trims / (EPOCH_SEC_SIZE * sampling_rate)))\n",
    "                select_idx = select_idx[:-n_trims]\n",
    "                labels = labels[:-n_label_trims]\n",
    "\n",
    "        # removing all unknown and movement labels\n",
    "        raw_ch = raw_ch_df.values[select_idx]\n",
    "\n",
    "        # check if we can split into epochs' size\n",
    "        if len(raw_ch) % (EPOCH_SEC_SIZE * sampling_rate) != 0:\n",
    "            raise Exception(\"Please choose anothe epoch duration!\")\n",
    "        n_epochs = len(raw_ch) / (EPOCH_SEC_SIZE * sampling_rate)\n",
    "\n",
    "        # get epochs and their corresponding labels\n",
    "        x = np.asarray(np.split(raw_ch, n_epochs)).astype(np.float32)\n",
    "        y = labels.astype(np.int32)\n",
    "\n",
    "        assert len(x) == len(y)\n",
    "\n",
    "        # select on sleep periods\n",
    "        w_edge_mins = 30\n",
    "        nw_idx = np.where(y != label_dict[\"W\"])[0]\n",
    "        start_idx = nw_idx[0] - (w_edge_mins * 2)\n",
    "        end_idx = nw_idx[-1] + (w_edge_mins * 2)\n",
    "        if start_idx < 0: start_idx = 0\n",
    "        if end_idx >= len(y): end_idx = len(y) - 1\n",
    "        select_idx = np.arange(start_idx, end_idx+1)\n",
    "        x = x[select_idx]\n",
    "        y = y[select_idx]\n",
    "\n",
    "        # file structure for saving\n",
    "        save_dict = {\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"fs\": sampling_rate,\n",
    "            \"ch_label\": ch_labels,\n",
    "            \"header_raw\": head_raw,\n",
    "            \"header_annotation\": annot_raw,\n",
    "        }\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        np.savez(os.path.join(output_path, filename), **save_dict)"
   ],
   "id": "1fba0de794364000"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pdf_report = PdfPages('Sleep_Plots.pdf')\n",
    "for item in tqdm(subject_files[:20]):  # selecting the first 20 subjects\n",
    "    raw_train = mne.io.read_raw_edf(item[0], verbose=VBS)\n",
    "    annot_train = mne.read_annotations(item[1])  # reading the hypnogram files\n",
    "\n",
    "    raw_train.set_annotations(annot_train, emit_warning=False)\n",
    "    raw_train.set_channel_types(mapping)\n",
    "    figure = plt.figure()\n",
    "    figure.suptitle(\"Subject: \" + str(item[0][-13:-11]) + \", Night: \" + str(item[0][-11:-10]))\n",
    "    pdf_report.savefig(figure, papertype='a0', bbox_inches='tight')\n",
    "    pdf_report.savefig(raw_train.plot(duration=30), papertype='a0', bbox_inches='tight')\n",
    "\n",
    "    annotation_desc_2_event_id = {'Sleep stage R': -1,\n",
    "                                  'Sleep stage W': 1,\n",
    "                                  'Sleep stage 1': 2,\n",
    "                                  'Sleep stage 2': 3,\n",
    "                                  'Sleep stage 3': 4,\n",
    "                                  'Sleep stage 4': 4\n",
    "    }\n",
    "\n",
    "    events_train, _ = mne.events_from_annotations(\n",
    "        raw_train, event_id=annotation_desc_2_event_id, chunk_duration=30.)\n",
    "\n",
    "    # create a new event_id that unifies stages 3 and 4\n",
    "    event_id = {'Sleep stage R'  : -1,\n",
    "                'Sleep stage W'  : 1,\n",
    "                'Sleep stage 1'  : 2,\n",
    "                'Sleep stage 2'  : 3,\n",
    "                'Sleep stage 3/4': 4\n",
    "    }\n",
    "    # plot events and saving them in PDF report\n",
    "    pdf_report.savefig(mne.viz.plot_events(events_train, event_id=event_id,\n",
    "                        sfreq=raw_train.info['sfreq']), bbox_inches='tight')\n",
    "pdf_report.close()"
   ],
   "id": "1252d1e04369a6d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from hurst import compute_Hc\n",
    "Fs= 100\n",
    "band_list = [0.5,4,7,12,30]\n",
    "PSD = []  # Power Spectral Density\n",
    "PFD = []  # Petrosian Fractal Dimension\n",
    "hjorths = []  # Hjorth Parameters\n",
    "hursts = []  # Hurst Exponent\n",
    "DFA = []  # Detrended Fluctuation Analysis\n",
    "for item in tqdm(subject_files[:5]):\n",
    "    raw_test = mne.io.read_raw_edf(item[0], verbose=False)\n",
    "    signals_list = raw_test[0][0][0]\n",
    "    first_order = np.diff(signals_list).tolist()\n",
    "    PSD.append(pyeeg.bin_power(signals_list, band_list, Fs))\n",
    "    PFD.append(pyeeg.pfd(signals_list, first_order))\n",
    "    hjorths.append(pyeeg.hjorth(signals_list, first_order))\n",
    "    hursts.append(compute_Hc(signals_list, kind='change', min_window=100))\n",
    "    DFA.append(pyeeg.dfa(signals_list))"
   ],
   "id": "34c74a7bd3017e91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if VBS:\n",
    "    print(\"Petrosian Fractal Dimension (PFD): \", PFD)\n",
    "    print(\"Hjorth mobility and complexity: \", hjorths)\n",
    "    print(\"Detrended Fluctuation Analysis (DFA): \", DFA)\n",
    "    print(\"Hurst Exponent (Hurst): \", hursts)"
   ],
   "id": "99e0b989c6dd2408"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "npz_files = sorted(glob.glob(os.path.join(output_path, \"*.npz\")))\n",
    "X = np.zeros((0, 3000, 1))\n",
    "y = []\n",
    "for fn in tqdm(npz_files[:number_of_subj]):\n",
    "    samples = np.load(fn)\n",
    "    X_data = samples['x']\n",
    "    X = np.concatenate((X, X_data), axis=0)\n",
    "    y.extend(samples['y'])\n",
    "y = np.array(y)"
   ],
   "id": "2129e56730c85f5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pd.Series(y).value_counts().plot.bar()\n",
    "plt.title(\"Frequency of the labels in our dataset\")"
   ],
   "id": "474f8a70c360176d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if VBS:\n",
    "    print(\"Shape of the input data: {}\".format(X.shape))\n",
    "    print(\"Shape of the sleep stages: {}\".format(y.shape))\n",
    "# splitting subjects\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=seed)\n",
    "# splitting sleeping signals\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=seed)\n",
    "if VBS:\n",
    "    print(\"Shape of the training dataset:\\ntraining dataset: {}\\ntest_dataset: {}\\n\"\n",
    "          .format(X_train.shape, X_test.shape))\n",
    "y_train_ = to_categorical(y_train)\n",
    "y_val_ = to_categorical(y_val)\n",
    "y_test_ = to_categorical(y_test)\n",
    "\n",
    "# X_train = np.squeeze(X_train)\n",
    "# X_test = np.squeeze(X_test)\n",
    "# X_val = np.squeeze(X_val)"
   ],
   "id": "f70c13a895c3e075"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pp_X_train = np.array([models.butter_bandpass_filter(sample, highpass=40.0, fs=100, order=4) for sample in X_train])\n",
    "pp_X_val = np.array([models.butter_bandpass_filter(sample, highpass=40.0, fs=100, order=4) for sample in X_val])\n",
    "pp_X_test = np.array([models.butter_bandpass_filter(sample, highpass=40.0, fs=100, order=4) for sample in X_test])\n",
    "# pp_X_test = np.expand_dims(pp_X_test, axis=2)\n",
    "# pp_X_train = np.expand_dims(pp_X_train, axis=2)\n",
    "# pp_X_val = np.expand_dims(pp_X_val, axis=2)\n",
    "if VBS:\n",
    "    print(pp_X_val.shape)\n",
    "    print(pp_X_train.shape)"
   ],
   "id": "957ca66dd5cb2112"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "checkpoint = ModelCheckpoint(\"model_cps\", monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_loss\", mode=\"max\", patience=5, verbose=2)\n",
    "csv_logger = CSVLogger('log_training.csv', append=True, separator=',')\n",
    "callbacks_list = [\n",
    "    checkpoint,\n",
    "    redonplat,\n",
    "    csv_logger,\n",
    "]"
   ],
   "id": "1de7fd2114ecbb41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_cnn = models.model_b(verbose=VBS)\n",
    "hist_19 = model_cnn.fit(\n",
    "    pp_X_train, y_train_, batch_size=batch_size, epochs=30, validation_data=(pp_X_val, y_val_), callbacks=callbacks_list, verbose=VBS\n",
    ")"
   ],
   "id": "dded911f99fd1bb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_pred = model_cnn.predict(pp_X_test, batch_size=batch_size)\n",
    "y_pred = np.array([np.argmax(s) for s in y_pred])\n",
    "f1_cnn = f1_score(y_test, y_pred, average=\"macro\")\n",
    "if VBS:\n",
    "    print(\"F1 score: {}\".format(f1_cnn))\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report)"
   ],
   "id": "71a1754076766379"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(hist_19.history[\"loss\"])\n",
    "plt.plot(hist_19.history[\"val_loss\"])"
   ],
   "id": "2dfaedfbbc0465ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(hist_19.history[\"acc\"])\n",
    "plt.plot(hist_19.history[\"val_acc\"])"
   ],
   "id": "40bf6f6142bf956f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ],
   "id": "f1c3569f0524d1df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# logging for visual comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "X_train_squeezed = np.squeeze(X_train)\n",
    "X_test_squeezed = np.squeeze(X_test)\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train_squeezed, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    if VBS:\n",
    "        print(\"-\"*27)\n",
    "        print(name)\n",
    "        print('----------Results----------')\n",
    "    train_predictions = clf.predict(X_test_squeezed)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    if VBS:\n",
    "        print(\"Accuracy: {:.2%}\".format(acc))\n",
    "    train_predictions = clf.predict_proba(X_test_squeezed)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    if VBS:\n",
    "        print(\"Log Loss: {:.2f}\".format(ll))\n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    log = log.append(log_entry)"
   ],
   "id": "651ee841c753e1bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pdf_report2 = PdfPages('Classifiers_Results.pdf')\n",
    "fig1 = plt.figure()\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n",
    "plt.xlabel('Accuracy %')\n",
    "plt.title('Classifier Accuracy')\n",
    "pdf_report2.savefig(fig1, papertype='a0', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig2 = plt.figure()\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Log Loss', y='Classifier', data=log, color=\"r\")\n",
    "plt.xlabel('Log Loss')\n",
    "plt.title('Classifier Log Loss')\n",
    "pdf_report2.savefig(fig2, papertype='a0', bbox_inches='tight')\n",
    "plt.show()\n",
    "pdf_report2.close()"
   ],
   "id": "47701cc07c7ccfb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "svm_wo_pca = SVC(kernel='rbf')\n",
    "svm_wo_pca.fit(X_train, y_train)"
   ],
   "id": "a013fb39a4465410"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "e_pred_svm_wo_pca = svm_wo_pca.predict(X_test)\n",
    "e_pred_prob_svm_wo_pca = svm_wo_pca.decision_function(X_test)"
   ],
   "id": "e6e1515ed974364f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print(svm_wo_pca.score(X_test_r, e_test))"
   ],
   "id": "e73d9c7d5a9136e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
